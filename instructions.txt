Lab assignment 2: Multilayer perceptron for
 classification problems
 Academic year 2025/2026
 Subject: Introduction to computational models
 4th course Computer Science Degree (University of C´ordoba)
 14th October 2025
 Abstract
 This lab assignment serves as familiarisation for the student with neural network computa
tional models applied to classification problems, in particular, with the multilayer perceptron
 implemented in the previous lab assignment. On the other hand, it is required to implement
 the off-line version of the training algorithm. The student must implement these modifications
 andchecktheeffect of different parameters over a given set of real-world datasets, with the aim
 of obtaining the best possible results in classification. Delivery will be made using the task in
 Moodle authorized for this purpose. All deliverables must be uploaded in a single compressed
 f
 ile indicated in this document. The deadline for the submission is 3th November 2025. In case
 two students submit copied assignments, neither of them will be scored.
 1 Introduction
 The worktobedoneinthis lab assignment consists on adapting the back-propagation algorithm
 implemented in the previous lab assignment to classification problems. Concretely, a probabilis
tic meaning will be given to this algorithm by means of two elements:
 • Useofthe softmax activation function in the output layer.
 • Useofthe cross-entropy error function.
 Furthermore, the off-line version of the algorithm will also be implemented.
 The student should develop a programme able to train a model with the aforementioned
 modifications. This programme will be used to train models able to classify as accurate as pos
sible a set of databases available in Moodle. Also, an analysis about the obtained results will be
 included. This analysis will greatly influence the qualification of this assignment.
 In the statement of the assignment, indicative values are provided for all parameters. How
ever, it will be positively evaluated if the student finds other values for these parameters able
 to achieve better results. The only condition is that the maximum number of iterations for the
 outer loop cannotbemodified(establishedto1000iterations for the XORproblemandProPublica
 COMPAS,and500 iterations for the noMNIST dataset).
 Section 2 describes a series of general guidelines when implementing the back-propagation
 algorithm. Section 4 explains the experiments to be carried out once the algorithm’s modifica
tions are implemented. Finally, section 5 specifies the files to be delivered for this assignment.
 2 Implementation of the back-propagation algorithm
 Follow the instructions on the class slides in order to add the following characteristics to the
 algorithm implemented in the previous lab assignment:
 1
1. Softmax function: The possibility to use the softmax function in the neurons of the output
 layer should be incorporated, being its output defined as:
 nH−1
 netH
 j = wH
 j0 +
 i=1
 wH
 jioutH−1
 i
 outH
 j = oj = exp(netH
 j )
 ,
 nH
 l=1 exp(netH
 l ) .
 (1)
 (2)
 You should implement the softmax to avoid number overflow. This can be easily fixed by
 substracting the maximum output to all the outputs before calculating the exponential (see
 details in the following section).
 2. Error function based on the cross-entropy: The possibility to use the cross-entropy as error
 function must be introduced, this is:
 L =−1
 N 
N
 p=1
 1
 k 
k
 o=1
 dpoln(opo) ,
 (3)
 where N is the number of patterns of the database, k is the number of outputs, dpo is set
 to 1 if the pattern p belongs to the o class (and 0 otherwise) and opo is the probability value
 obtained by the model for the pattern p and the class o.
 3. Working mode: Apart from working in on-line mode (previous lab assignment), the algo
rithm should include the possibility of working in off-line mode or batch. This is, for each
 training pattern (inner loop), the error will be computed and the change accumulated, but
 we will not adjust the network weights. Once all the training patterns are processed (and
 the changes accumulated), then the weights will be adjusted and the stopping condition of
 the outer loop will be checked (in the case that the stopping condition is not satisfied, we
 will start again by the first pattern). Remember to average the derivates during the weight
 adjustment for the off-line mode, as it is explained in the slides.
 4. The rest of the algorithm characteristics (use of the training files and test files), stopping
 condition, copies of the weights and seeds for the random numbers) will be the same specified in
 the previous lab assignment. However, for this assignment, it is highly recommended to
 take the default values for the learning rate and the following momentum factors: η = 0.7
 and µ =1,adjusting them if necessary until convergence is achieved.
 It is recommended to implementtheprevious points, checking that everything work fine (at least
 with two datasets) before moving forward to the next point.
 3 Numericalstability and softmax
 This section is anadaptationofthesection“Computingsoftmaxandnumericalstability”in1. The
 numerical rangeoffloating-pointnumbersislimitedanditiseasytoexceedwithexponenciation.
 For double in C++, the maximal representable number is on the order of 10308.
 Toavoidoverflow, wenormalizetheinputssothattheyarenottoolargeortoosmall. Starting
 from the softmax definition for a hidden or output layer H:
 nH−1
 netH
 j = wH
 j0 +
 i=1
 wH
 ji outH−1
 i
 outH
 j = oj = exp(netH
 j )
 nH
 l=1 exp(netH
 l ) .
 ,
 1https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative
 (4)
 (5)
 2
Wecanmultiply the numerator and denominator by an arbitrary constant C:
 outH
 j =
 Pushing C into the exponent gives:
 outH
 j =
 C exp(netH
 j )
 nH
 l=1 C exp(netH
 l ) .
 exp(netH
 j + log(C))
 nH
 l=1 exp(netH
 l + log(C)) .
 Since C is arbitrary, we can instead write D = log(C), leading to:
 outH
 j =
 exp(netH
 j + D)
 nH
 l=1 exp(netH
 l + D) .
 (6)
 (7)
 (8)
 This form is equivalent to the original softmax for any D. To improve numerical stability, we
 choose D such that it minimizes the chance of overflow. A good choice is to set D to the negative
 of the maximum activation among all units in layer H:
 D=−max
 l
 (netH
 l ).
 (9)
 This transformation shifts all inputs so that the largest exponent becomes exp(0) = 1, pre
venting overflow and keeping computations stable.
 4 Experiments
 We will test different configurations of the neural network and execute each configuration with
 f
 ive seeds (1, 2, 3, 4 and 5). Based on the results obtained, the average and standard deviation
 of the error will be obtained. Although the training is guided by the cross-entropy or the MSE,
 the programme must show the percentage of correct classified patterns (CCR), given that for
 classification problems this is the most appropriate performance measure. 2 The percentage of
 correct classified patterns can be expressed as follows:
 CCR=100× 1
 N 
N
 p=1
 I(yp = y∗
 p) ,
 (10)
 whereN isthenumberofpatternsofthedatasetconsidered, yp is the target class for the pattern p
 (this is, the index of the maximum value of the vector dp, yp = argmaxodpo, or what is the same,
 the index of the position with a 1) and y∗
 p is the class obtained for the pattern p (this is, the index
 of the maximumvalueofthevector op or the output neuron that achieves the highest probability
 for the pattern p, y∗
 p = argmaxo opo).
 To assess how the implemented algorithm works, we will run it on three different datasets:
 • XOR problem: this dataset represents the problem of non-linear classification of the XOR.
 The same file will be used for train and test. As can be seen, this file has been adapted to
 1-to-k codification, finding two outputs instead of one.
 • ProPublica COMPAS: This dataset is about the performance of COMPAS algorithm, a sta
tistical method for assigning risk scores within the United States criminal justice system
 created by Northpointe. It was published by ProPublica in 2016 3, claiming that this risk
 tool was biased against African-American individuals (we will deal with this in the fol
lowing assignment). In this dataset, they analyzed the COMPAS scores for “risk of recidi
vism” so each individual has a binary “recidivism” outcome, that is the prediction task,
 2The bad thing is that it is not derivable and we can not use it to adjust weights.
 3https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm
 3
indicating whether they were rearrested within two years after the first arrest (the charge
 described in the data). We reduced the original dataset from 52 to 9 attributes similarly to
 the original dataset: sex, age, age cat, race, juv fel count, juv misd count, juv other count,
 priors count, c charge degree. The prediction variable is whether the individual will be
 rearrested in two years or not.
 1. sex: binary sex.
 2. age: numerical age.
 3. age cat: categorical (age < 25, age ≥ 25 and age < 45, age ≥ 45).
 4. race: binary attribute (0 means ’white’ and 1 means ’black’).
 5. juv fel count: a continuous variable containing the number of juvenile felonies.
 6. juv misd count: acontinuousvariablecontainingthenumberofjuvenilemisdemeanors.
 7. juv other count: a continuous variable containing the number of prior juvenile con
victions that are not considered either felonies or misdemeanors.
 8. priors count: a continuous variable containing the number of prior crimes committed.
 9. c
 charge degree: Degree of the crime. It is either M (Misdemeanor), F (Felony), or O
 (not causing jail time).
 • noMNIST dataset: originally, this dataset was composed by 200.000 training patterns and
 10.000 test patterns, with a total of 10 classes. Nevertheless, for this lab assignment, the size
 of the dataset has been reduced in order to reduce the computational cost. In this sense,
 the dataset is composed by 900 training patterns and 300 test patterns. It includes a set of
 letters (from a to f) written with different typologies or symbols. They are adjusted to a
 squared grid of 28 × 28 pixels. The images are in grey scale in the interval [−1.0;+1.0]4.
 Each of the pixels is an input variable (with a total of 28 × 28 = 784 input variables) and
 the class corresponds to a written letter (a, b, c, d, e y f, with a total of 6 classes). Figure
 1 represents a subset of 180 training patterns, whereas figure 2 represents a subset of 180
 letters from the test set. Moreover, all the letters are arranged and available in Moodle
 in the files train
 img
 nomnist.tar.gzandtest
 img
 nomnist.tar.gz,respectively.
 As youcancheck in the sample executions provided in Section 5.2, the results are not good
 for this dataset when you consider the “on-line” mode. You are asked to improve these
 results by considering the following hint: the order of the patterns is very important for
 on-line learning given that one type of patterns can be forgot when it is not shown to the
 network for a long time.
 Figure 1: Subset of letters belonging to the training dataset.
 All the databases are normalized. Outputs are never normalised in classification.
 Atable for each dataset must be built, comparing the average and standard deviation of the
 following four measures:
 4Check http://yaroslavvb.blogspot.com.es/2011/09/notmnist-dataset.html for more information.
 4
Figure 2: Subset of letters belonging to the test dataset.
 • Training and test errors. The MSE or the cross-entropy will be used, according to the
 decision made by the user to adjust the weights.
 • Training and test CCR.
 The momentum factor must always be used. It is highly recommended to use the values η =
 0.7 y µ = 1, adjusting them if necessary until convergence is achieved. At least, the following
 configuration must be tested:
 • Network architecture: For this first run, use the cross-entropy error function and the softmax
 activation function in the output layer, using the off-line version of the algorithm.– For the XOR problem, use the architecture achieving the best performance in the pre
vious lab assignment.– FortheProPublicaandnoMNISTproblems,8differentarchitectures(oneortwohidden
 layers with 4, 8, 16 o 64 neurons) must be tested.
 • Once decided the best architecture, test the following combinations (with the off-line algo
rithm):– MSEerrorfunction and sigmoidal activation function in the output layer.– MSEerrorfunction and softmax activation function in the output layer.– Cross-entropy error function and softmax activation function in the output layer.– Do not try the combination of cross-entropy error function and sigmoidal activation
 function in the output layer, given that it will lead to a bad performance (explain why).
 • Once decided the best combination (achieved using the off-line version of the algorithm),
 compare the results against the on-line version of the algorithm.
 Attention: Depending on the error function, it could be necessary to adapt the values for the
 learning rate (η) and the momentum factor (µ).
 As a guideline, the training CCR and the test CCR achieved by a logistic regression (using
 Weka) over the three datasets is shown:
 • XORproblem: CCRtraining = CCRtest = 50%.
 • ProPublica dataset: CCRtraining = 67.7348%;CCRtest = 66.8514%.
 • noMNISTdataset: CCRtraining = 80.4444%;CCRtest = 82.6667%.
 The student should be able to improve this error values with some of the configurations.
 4.1 File format
 The datasets files will follow the same format than the previous assignment. Note that for this
 lab assignment, all the files have multiple outputs (one for each class).
 5
5 Assignments
 The files to be submitted will be the following:
 • Report in a pdf file describing the programme implemented, including results, tables and
 their analysis.
 • Executable file and source code.
 5.1 Report
 The report for this lab assignment must include, at least, the following content:
 • Cover with the lab assignment number, its title, subject, degree, faculty department, uni
versity, academic year, name, DNI and email of the student
 • Index of the content with page numbers.
 • Descriptionoftheneuralnetworkmodelsused(architectureandlayerorganisation)(1page
 maximum).
 • Pseudocode description of the back-propagation algorithm and all those relevant opera
tions. The pseudocode must necessarily reflect the implementation and development done
 and not a generic description extracted from the slides or any other source. (3 pages maxi
mum).
 • Experiments and results discussion:– Brief description of the datasets used.– Brief description of the values of the parameters considered.– Results obtained, according to the format specified in the previous section.– Discussion/analysis of the results. The analysis must be aimed at justifying the results
 obtained instead of merely describing the tables. Take into account that this part is
 extremely decisive in the lab assignment qualification. The inclusion of the following
 comparison items will be appreciated:
 * Test confusion matrix of the best neural network model achieved for the noMNIST
 database.
 * Also for noMNIST, analyse the errors, including the images of some letters for
 which the model mistakes, to visually check if they are confusing.
 * Convergence charts: they reflect, on the x-axis, the iteration number of the algo
rithms, and, in the y-axis, the CCR on the training set and/or on the test set.
 • Bibliographic references or any other material consulted in order to carry out the lab as
signment different to the one provided by the lecturers (if any).
 Although the content is important, the presentation, including the style and structure of the
 documentwillalsobevalued. Thepresenceoftoomanyspellingmistakescandecreasethegrade
 obtained.
 5.2 Executable and source code
 Together with the report, the executable file prepared to be run in the UCO’s machines (con
cretely, test using ssh on ts.uco.es) must be included. In addition, all the source code must
 be included. The executable should have the following characteristics:
 • Its name will be la2.
 6
• The programme to be developed receive twelve arguments on command line (that could
 appear in any order). 5 The first nine arguments have not changed with respect to the
 previous assignment. The last three arguments incorporate the modifications included in
 this assignment:– Argument t: Indicates the name of the file that contains the training data to be used.
 This argument is compulsory, and without it, the program can not work.– Argument T: Indicates the name of the file that contains the testing data to be used. If
 it is not specified, training data will be used as testing data.– Argumenti: Indicates the number of iterations for the outer loop. If it is not specified,
 use 1000 iterations.– Argument l: Indicates the number of hidden layers of the neural network. If it is not
 specified, use 1 hidden layer.– Argument h: Indicates the number of neurons to be introduced in each hidden layer.
 If it is not specified, use 5 neurons.– Argument e: Indicates the value for the eta (η) parameter. By default, use η = 0.1.– Argument m: Indicates the value for the mu (µ) parameter. By default, use µ = 0.9.– Argument o: Boolean that indicates if the on-line version is applied. By default, use
 the off-line version.– Argument f: Indicates the error function to be used (0 for the MSE and 1 for the
 cross-entropy). By default, use MSE.– Arguments: Booleanthat indicates if the softmax function is used for the output layer.
 By default, use the sigmoidal function.– Argument n: Boolean indicating that the data (training and test) will be normalised
 after reading. Inputs will be normalised to the interval [−1,1] (outputs are not nor
malised in classification).
 • Optionally, another argument could be included to save the configuration of the trained
 model (it would be necessary to obtain the predictions for the Kaggle competition):– Argument w: Indicates the name of the file in which the configuration will be stored
 and the value of the weights of the trained model.
 • Anexampleofexecution can be seen in the following output:
 1
 2
 3
 i02gupep@VTS1:˜/imc/imc2526/la2$ make
 mkdir-p bin/./
 4 g++-I.-MMD-MP-O3-c la2.cpp-o bin/./la2.cpp.o
 5
 mkdir-p bin/./imc/
 6 g++-I.-MMD-MP-O3-c imc/MultilayerPerceptron.cpp-o bin/./imc/
 MultilayerPerceptron.cpp.o
 7
 mkdir-p bin/./imc/
 8 g++-I.-MMD-MP-O3-c imc/util.cpp-o bin/./imc/util.cpp.o
 9 g++ ./bin/./la2.cpp.o ./bin/./imc/MultilayerPerceptron.cpp.o ./bin/./imc/util.cpp.o-o bin/la2
 10
 11
 12
 13
 i02gupep@VTS1:˜/imc/imc2526/la2$ make run
 **********
 14 SEED 1
 15
 16
 17
 18
 **********
 Iteration 1
 Iteration 2
 Iteration 3
 Training error: 0.366974
 Training error: 0.428928
 Training error: 0.359314
 5Use the function getopt() from libc to process the input sequence.
 7
19 Iteration4 Trainingerror:0.404694
 20 Iteration5 Trainingerror:0.354386
 21 Iteration6 Trainingerror:0.38935
 22 Iteration7 Trainingerror:0.351464
 23 Iteration8 Trainingerror:0.376896
 24 Iteration9 Trainingerror:0.348207
 25 Iteration10 Trainingerror:0.367123
 26
 27 ....
 28
 29 Iteration434 Trainingerror:0.00241054
 30 Weexitbecausethetrainingisnotimproving!!
 31 Iteration1001 Trainingerror:0.00240319
 32 NETWORKWEIGHTS
 33 ===============
 34 Layer1
 35-----
36 1.767564-1.5826931.792149
 37 0.323078-0.122917-0.267473
 38-0.4844660.502920-0.641518
 39-0.1198680.7364930.049361
 40-1.5477811.7424771.494000
 41 1.7606521.4213691.671825
 42-2.8294742.692585-2.850542
 43-1.912894-2.1884271.976265
 44-2.510571-2.280180-2.489807
 45-1.5859841.9171691.509418
 46-0.1910930.9921900.161791
 47-1.6074411.8435751.525103
 48 2.2159342.468364-2.276200
 49 2.0176171.7391671.982108
 50-2.2436472.093026-2.272617
 51 0.6611160.3095710.946579
 52 Layer2
 53-----
54-1.8347870.6039880.150670-0.830812-2.2577522.1607564.5812383.294247-3.682223-2.623108-0.673258-2.579435-3.8495572.9188603.0763201.045359
 0.188493
 55 DesiredoutputVsObtainedoutput(test)
 56 =========================================
 57 1--0.9943110--0.00568886
 58 0--0.004027351--0.995973
 59 1--0.9958470--0.00415294
 60 0--0.005309231--0.994691
 61 Weend!!=>FinaltestCCR:100
 62 **********
 63 SEED2
 64 **********
 65 Iteration1 Trainingerror:0.373712
 66 Iteration2 Trainingerror:0.446452
 67 Iteration3 Trainingerror:0.382052
 68 Iteration4 Trainingerror:0.403883
 69 Iteration5 Trainingerror:0.383328
 70 Iteration6 Trainingerror:0.37884
 71 Iteration7 Trainingerror:0.380041
 72 Iteration8 Trainingerror:0.363258
 73 Iteration9 Trainingerror:0.374255
 74
 75 ....
 76
 77 Iteration453 Trainingerror:0.00248692
 78 Weexitbecausethetrainingisnotimproving!!
 79 Iteration1001 Trainingerror:0.00247953
 80 NETWORKWEIGHTS
 81 ===============
 82 Layer1
 83-----
8
84-2.4340462.424894-2.462230
 85-0.761392-0.147430-0.005780
 86 1.877855-1.903325-1.879409
 87 1.2015771.373681-1.217738
 88 1.672072-1.6945221.731875
 89-2.230475-2.2638872.232353
 90 0.694545-0.478623-0.257903
 91-1.5442681.565481-1.604283
 92-0.3650590.8463690.679439
 93-2.368988-2.339782-2.348910
 94-2.7189942.7289952.721820
 95 0.1801470.799231-0.531017
 96-2.1816282.166651-2.205463
 97 2.049371-2.060424-2.054738
 98-0.9588100.383526-0.544686
 99 1.4378801.3737831.280033
 100 Layer2
 101-----
102 3.9026150.2100672.631572-1.439580-2.2700113.3494880.2460821.927923-0.436802-3.665715-4.924741-0.1617293.2139022.9575820.3363331.416962-0.751096
 103 DesiredoutputVsObtainedoutput(test)
 104 =========================================
 105 1--0.9949290--0.00507098
 106 0--0.004978771--0.995021
 107 1--0.9953150--0.00468487
 108 0--0.005052441--0.994948
 109 Weend!!=>FinaltestCCR:100
 110 **********
 111 SEED3
 112 **********
 113 Iteration1 Trainingerror:0.505981
 114
 115 ....
 116
 117 **********
 118 SEED4
 119 **********
 120
 121 ....
 122
 123 **********
 124 SEED5
 125 **********
 126
 127 ....
 128
 129 Iteration459 Trainingerror:0.00261932
 130 Iteration460 Trainingerror:0.00261179
 131 Weexitbecausethetrainingisnotimproving!!
 132 Iteration1001 Trainingerror:0.00260431
 133 NETWORKWEIGHTS
 134 ===============
 135 Layer1
 136-----
137 1.766247-1.7381411.722196
 138-2.723600-2.745772-2.777451
 139-0.464393-0.0993360.005649
 140 2.123664-2.165483-2.119360
 141 2.112435-2.0649092.035235
 142-0.261113-0.109613-0.144188
 143 0.096265-0.530093-0.353834
 144-0.825735-0.882058-1.014923
 145-1.3301091.3450561.391301
 146-1.524216-1.4924461.583586
 147-0.0468630.299682-0.965822
 148 2.2371422.213214-2.241833
 149-0.255536-1.0067400.316202
 9
150
 151
 152
 153
 154
 155-2.529145 2.498247-2.470761
 0.774923 0.964924 1.035262
 2.804878-2.833998-2.802736
 Layer 2-------2.253874-4.558486 0.266875 3.133218-2.992188 0.993913 0.449825-1.068333-1.301015 2.116415 0.102893-3.466655 0.573371 4.273376 1.085973 4.997441
 0.378361
 Desired output Vs Obtained output (test)
 =========================================
 158 1-- 0.994717 0-- 0.00528311
 159 0-- 0.00503699 1-- 0.994963
 160 1-- 0.994611 0-- 0.00538929
 161 0-- 0.0050709 1-- 0.994929
 162 We end!! => Final test CCR: 100
 163 WE HAVE FINISHED WITH ALL THE SEEDS
 FINAL REPORT
 156
 157
 164
 165
 166
 *************
 Train error (Mean +- SD): 0.0025534 +- 0.000133471
 167 Test error (Mean +- SD): 0.0025534 +- 0.000133471
 Train CCR (Mean +- SD): 100 +- 0
 169 Test CCR (Mean +- SD): 100 +- 0
 168
 170
 171
 172
 173
 174
 ___________________________________________________________________________________
 i02gupep@VTS1:˜/imc/imc2526/la2$ bin/la2-t ./dat/train_nomnist.dat-T ./dat/
 test_nomnist.dat-i 500-l 1-h 4-f 1-s
 175 ....
 FINAL REPORT
 176
 177
 178
 *************
 Train error (Mean +- SD): 0.069922 +- 0.00569867
 179 Test error (Mean +- SD): 0.123385 +- 0.0128565
 Train CCR (Mean +- SD): 89 +- 0.906084
 181 Test CCR (Mean +- SD): 80.6667 +- 1.5456
 180
 182
 183
 184
 185
 ___________________________________________________________________________________
 i02gupep@VTS1:˜/imc/imc2526/la2$ bin/la2-t ./dat/train_nomnist.dat-T ./dat/
 test_nomnist.dat-i 500-l 1-h 4-e 1-m 2-f 1-s
 186 ....
 FINAL REPORT
 187
 188
 189
 *************
 Train error (Mean +- SD): 0.0485762 +- 0.00479099
 190 Test error (Mean +- SD): 0.114138 +- 0.0122826
 Train CCR (Mean +- SD): 92.0889 +- 0.557773
 192 Test CCR (Mean +- SD): 83.4 +- 1.36219
 191
 193
 194
 195
 196
 197
 198
 ___________________________________________________________________________________
 i02gupep@VTS1:˜/imc/imc2526/la2$ bin/la2-t ./dat/train_nomnist.dat-T ./dat/
 test_nomnist.dat-i 500-l 1-h 4-e 1-m 2-f 0-s
 199
 200 ....
 201
 202
 203
 204
 FINAL REPORT
 *************
 Train error (Mean +- SD): 0.0390912 +- 0.00594294
 205 Test error (Mean +- SD): 0.0544008 +- 0.00626936
 Train CCR (Mean +- SD): 85.0222 +- 3.20243
 207 Test CCR (Mean +- SD): 77.5333 +- 4.09336
 206
 208
 209
 210
 211
 ___________________________________________________________________________________
 10
212 i02gupep@VTS1:˜/imc/imc2526/la2$bin/la2-t./dat/train_nomnist.dat-T./dat/
 test_nomnist.dat-i500-l1-h8-e0.1-m2-f1-s-o
 213 ....
 214
 215 FINALREPORT
 216 *************
 217 Trainerror(Mean+-SD):0.257788+-0.0314335
 218 Testerror(Mean+-SD):0.254275+-0.0394237
 219 TrainCCR(Mean+-SD):53.4667+-7.09521
 220 TestCCR(Mean+-SD):55.1333+-7.05376
 221
 222 ___________________________________________________________________________________
 223
 224 i02gupep@VTS1:˜/imc/imc2526/la2$bin/la2-t./dat/train_compas.dat-T./dat/
 test_compas.dat
 225 ...
 226 FINALREPORT
 227 *************
 228 Trainerror(Mean+-SD):0.207383+-0.000352453
 229 Testerror(Mean+-SD):0.211912+-0.000619694
 230 TrainCCR(Mean+-SD):68.5482+-0.219083
 231 TestCCR(Mean+-SD):67.9601+-0.214689
 5.3 [OPTIONAL]ObtainingthepredictionsforKaggle
 Thesameexecutableoftheassignmentwillallowobtainingthepredictionsforagivendataset.
 Thisoutputmustbesavedina.csvfilethatmustbeuploadedtoKaggletoparticipate in
 thecompetition(checkthefileformatofsampleSubmission.csvonKaggle).Thisprediction
 modeusesdifferentparameterthanthosementionedpreviously:
 •Argumentp:Flagindicatingthattheprogramwillruninpredictionmode.
 •ArgumentT:Indicatesthenameofthefilecontainingthetestdatatobeused(testkaggle.dat).
 •Argumentw: Indicatesthenameofthefilecontainingtheconfigurationandthevaluesfor
 theweightsofthetrainedmodelthatwillbeusedtopredicttheoutputs.
 Belowisanexampleofhowthetrainingmodeisexecutedusingtheparameterw,whichsaves
 theconfigurationofthemodel.
 1 i02gupep@NEWTS:˜/imc/workspace/la2/Debug$./la2-t../train.dat-T../val.dat-i1000-l
 1-h20-e1.5-m1-f1-s-wweights.txt
 2
 3 **********
 4 SEED1
 5 **********
 6
 7 ...
 8
 9 **********
 10 SEED2
 11 **********
 12
 13 ...
 14
 15 **********
 16 SEED3
 17 **********
 18
 19 ...
 20
 21 **********
 22 SEED4
 23 **********
 11
24
 25 ...
 26
 27 **********
 28 SEED5
 29 **********
 30
 31 ...
 32
 33 FINALREPORT
 34 *************
 35 Trainerror(Mean+-SD):0.110211+-0.000890431
 36 Testerror(Mean+-SD):0.162645+-0.00309483
 37 TrainCCR(Mean+-SD):41.475+-1.07674
 38 TestCCR(Mean+-SD):23.3+-1.98746
 Belowisanexampleoftheoutputusingthepredictionmode:
 1 i02gupep@NEWTS:˜/imc/practica1/Debug$./la2-T../test_X.dat-p-wweights.txt>
 sample_submission.csv
 2 i02gupep@NEWTS:˜/imc/practica1/Debug$headsample_submission.csv
 3 Id,Category
 4 0,13
 5 1,4
 6 2,9
 7 3,12
 8 4,10
 9 5,8
 10 6,11
 11 7,9
 12 8,9
 13 i02gupep@NEWTS:˜/imc/practica1/Debug$tailsample_submission.csv
 14 1702,7
 15 1703,14
 16 1704,9
 17 1705,9
 18 1706,8
 19 1707,8
 20 1708,2
 21 1709,11
 22 1710,8
 23 1711,13
 12